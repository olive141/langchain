{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "49e9c69e",
      "metadata": {
        "id": "49e9c69e"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/06-langchain-agents.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/langchain/handbook/06-langchain-agents.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1571b632",
      "metadata": {
        "id": "1571b632"
      },
      "source": [
        "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
        "\n",
        "# Agents 🤖\n",
        "\n",
        "Agents are like \"tools\" for LLMs. They allow a LLM to access Google search, perform complex calculations with Python, and even make SQL queries.\n",
        "\n",
        "In this notebook we'll explore agents and how to use them in LangChain.\n",
        "\n",
        "We'll start by installing the prerequisite libraries that we'll be using in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "uZR3iGJJtdDE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZR3iGJJtdDE",
        "outputId": "196a7da0-6099-4721-d8bc-9247fb852620"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain openai google-search-results wikipedia sqlalchemy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "wPdWz1IdxyBR",
      "metadata": {
        "id": "wPdWz1IdxyBR"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c02c4fa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c02c4fa2",
        "outputId": "f5843a3b-2e86-4616-88a4-e85c828fb9ba"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import AzureOpenAI\n",
        "\n",
        "\n",
        "with open(r'..\\config.json') as config_file:\n",
        "    config_details = json.load(config_file)\n",
        "\n",
        "openai.api_base = config_details['OPENAI_API_BASE']\n",
        "openai.api_key = config_details['OPENAI_API_KEY']  \n",
        "openai.api_type = \"azure\"\n",
        "openai.api_version = \"2022-12-01\" \n",
        "\n",
        "\n",
        "# initialize the models\n",
        "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", openai_api_version=\"2022-12-01\", openai_api_key=openai.api_key, temperature=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "309g_2pqxzzB",
      "metadata": {
        "id": "309g_2pqxzzB"
      },
      "source": [
        "As we did before, we will be counting our tokens in each call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "DsC3szr6yP3L",
      "metadata": {
        "id": "DsC3szr6yP3L"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def count_tokens(agent, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = agent(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bd60a97c",
      "metadata": {
        "id": "bd60a97c"
      },
      "source": [
        "With all of that set up, let's jump into **Agents**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6e1f31b4",
      "metadata": {
        "id": "6e1f31b4"
      },
      "source": [
        "## What is an agent?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5b919c3a",
      "metadata": {
        "id": "5b919c3a"
      },
      "source": [
        "**Definition**: The key behind agents is giving LLM's the possibility of using tools in their workflow. This is where langchain departs from the popular chatgpt implementation and we can start to get a glimpse of what it offers us as builders. Until now, we covered several building blocks in isolation. Let's see them come to life.\n",
        "\n",
        "The official definition of agents is the following:\n",
        "\n",
        "\n",
        "> Agents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6c9c13e9",
      "metadata": {
        "id": "6c9c13e9"
      },
      "source": [
        "In this edition we will cover what we may call 'generic' agents which really able to perform many meta tasks. There are other more specific agents that are tuned for different tasks (called 'agent-toolkits'), but we will cover those in a future edition."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a93dc920",
      "metadata": {
        "id": "a93dc920"
      },
      "source": [
        "## Create database"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b601f903",
      "metadata": {
        "id": "b601f903"
      },
      "source": [
        "We will use the agents to interact with a small sample database of stocks. We will not dive into the details because this is just a dummy tool we will build for illustrative purposes. Let's create it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "61b1f17c",
      "metadata": {
        "id": "61b1f17c"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import MetaData\n",
        "\n",
        "metadata_obj = MetaData()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "3cc1d80e",
      "metadata": {
        "id": "3cc1d80e"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import Column, Integer, String, Table, Date, Float\n",
        "\n",
        "stocks = Table(\n",
        "    \"stocks\",\n",
        "    metadata_obj,\n",
        "    Column(\"obs_id\", Integer, primary_key=True),\n",
        "    Column(\"stock_ticker\", String(4), nullable=False),\n",
        "    Column(\"price\", Float, nullable=False),\n",
        "    Column(\"date\", Date, nullable=False),    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "c9a9571a",
      "metadata": {
        "id": "c9a9571a"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "engine = create_engine(\"sqlite:///:memory:\")\n",
        "metadata_obj.create_all(engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "81c3081f",
      "metadata": {
        "id": "81c3081f"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "observations = [\n",
        "    [1, 'ABC', 200, datetime(2023, 1, 1)],\n",
        "    [2, 'ABC', 208, datetime(2023, 1, 2)],\n",
        "    [3, 'ABC', 232, datetime(2023, 1, 3)],\n",
        "    [4, 'ABC', 225, datetime(2023, 1, 4)],\n",
        "    [5, 'ABC', 226, datetime(2023, 1, 5)],\n",
        "    [6, 'XYZ', 810, datetime(2023, 1, 1)],\n",
        "    [7, 'XYZ', 803, datetime(2023, 1, 2)],\n",
        "    [8, 'XYZ', 798, datetime(2023, 1, 3)],\n",
        "    [9, 'XYZ', 795, datetime(2023, 1, 4)],\n",
        "    [10, 'XYZ', 791, datetime(2023, 1, 5)],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "85fd20fa",
      "metadata": {
        "id": "85fd20fa"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import insert\n",
        "\n",
        "def insert_obs(obs):\n",
        "    stmt = insert(stocks).values(\n",
        "    obs_id=obs[0], \n",
        "    stock_ticker=obs[1], \n",
        "    price=obs[2],\n",
        "    date=obs[3]\n",
        "    )\n",
        "\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(stmt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "6766f1f7",
      "metadata": {
        "id": "6766f1f7"
      },
      "outputs": [],
      "source": [
        "for obs in observations:\n",
        "    insert_obs(obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "9721648e",
      "metadata": {
        "id": "9721648e"
      },
      "outputs": [],
      "source": [
        "from langchain.sql_database import SQLDatabase\n",
        "from langchain.chains import SQLDatabaseChain\n",
        "\n",
        "db = SQLDatabase(engine)\n",
        "sql_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fe2af8c8",
      "metadata": {
        "id": "fe2af8c8"
      },
      "source": [
        "Finally, we will create a tool with this chain. To create a custom tool we need only three inputs:\n",
        "* name: the name of the tool (sent to the llm as context)\n",
        "* func: the function that will be applied on the LLM's request\n",
        "* description: the description of the tool, what it should be used for (sent to the llm as context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "7ef519b1",
      "metadata": {
        "id": "7ef519b1"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import Tool\n",
        "\n",
        "sql_tool = Tool(\n",
        "    name='Stock DB',\n",
        "    func=sql_chain.run,\n",
        "    description=\"Useful for when you need to answer questions about stocks \" \\\n",
        "                \"and their prices.\"\n",
        "    \n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6325f758",
      "metadata": {
        "id": "6325f758"
      },
      "source": [
        "## Agent types"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4d732b7a",
      "metadata": {
        "id": "4d732b7a"
      },
      "source": [
        "In this section we will review several agents and see how they 'think' and what they can do."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8ee98631",
      "metadata": {
        "id": "8ee98631"
      },
      "source": [
        "Using one of langchain's pre-built agents involves three variables: \n",
        "* defining the tools\n",
        "* defining the llm\n",
        "* defining the agent type\n",
        "\n",
        "This is all really easy to do in langchain, as we will see in the following example."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1eaf306a",
      "metadata": {
        "id": "1eaf306a"
      },
      "source": [
        "### Agent type #1: Zero Shot React"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "13951c49",
      "metadata": {
        "id": "13951c49"
      },
      "source": [
        "We will be using only a few tools but you can combine the ones you prefer. Remember that tools are only utility chains under the hood which are chains that serve one specific purpose."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aabcff15",
      "metadata": {
        "id": "aabcff15"
      },
      "source": [
        "We first need to initialize the tools we'll be using in this example. Here we will want our agent to do math so we will use 'llm-math', the tool we have for solving math problems. This is one of the several tools we can load with the `load_tools` utility - you can check out more by checking out the [docs](https://langchain.readthedocs.io/en/latest/modules/agents/tools.html?highlight=tools#tools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "0c6bd290",
      "metadata": {
        "id": "0c6bd290"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "tools = load_tools(\n",
        "    [\"llm-math\"], \n",
        "    llm=llm\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bf01b924",
      "metadata": {
        "id": "bf01b924"
      },
      "source": [
        "To be able to search across our stock prices, we will also use the custom tool we built beforehand with the sql data ('Stock DB'). We will append this tool to our list of tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "e9d016d7",
      "metadata": {
        "id": "e9d016d7"
      },
      "outputs": [],
      "source": [
        "tools.append(sql_tool)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bc9bf6b5",
      "metadata": {
        "id": "bc9bf6b5"
      },
      "source": [
        "As the name suggests, we will use this agent to perform 'zero shot' tasks on the input. That means that we will not have several, interdependent interactions but only one. In other words, this agent will have no memory."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a6e51d51",
      "metadata": {
        "id": "a6e51d51"
      },
      "source": [
        "Now we are ready to initialize the agent! We will use `verbose` in `True` so we can see what is our agent's 'thinking' process."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ca2d95ce",
      "metadata": {
        "id": "ca2d95ce"
      },
      "source": [
        "**Important Note:** *When interacting with agents it is really important to set the `max_iterations` parameters because agents can get stuck in infinite loops that consume plenty of tokens. The default value is 15 to allow for many tools and complex reasoning but for most applications you should keep it much lower.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "5e4eb28c",
      "metadata": {
        "id": "5e4eb28c"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "zero_shot_agent = initialize_agent(\n",
        "    agent=\"zero-shot-react-description\", \n",
        "    tools=tools, \n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "28c239c0",
      "metadata": {
        "id": "28c239c0"
      },
      "source": [
        "Let's see our newly created agent in action! We will ask it a question that involves a math operation over the stock prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "9c775bc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c775bc6",
        "outputId": "979f31fc-c606-473d-deb3-3b1dc0222a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m I need to compare the stock prices of 'ABC' and 'XYZ' on two different days\n",
            "Action: Stock DB\n",
            "Action Input: Stock prices of 'ABC' and 'XYZ' on January 3rd and January 4th\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Stock prices of 'ABC' and 'XYZ' on January 3rd and January 4th\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT stock_ticker, price, date FROM stocks WHERE (stock_ticker = 'ABC' OR stock_ticker = 'XYZ') AND (date = '2023-01-03' OR date = '2023-01-04')\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[('ABC', 232.0, '2023-01-03'), ('ABC', 225.0, '2023-01-04'), ('XYZ', 798.0, '2023-01-03'), ('XYZ', 795.0, '2023-01-04')]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mOn January 3rd, the price of ABC was 232.0 and the price of XYZ was 798.0. On January 4th, the price of ABC was 225.0 and the price of XYZ was 795.0.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mOn January 3rd, the price of ABC was 232.0 and the price of XYZ was 798.0. On January 4th, the price of ABC was 225.0 and the price of XYZ was 795.0.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the ratio between the two stock prices on each day\n",
            "Action: Calculator\n",
            "Action Input: 232.0/798.0 and 225.0/795.0\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 0.08228117463469994\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the multiplication of the two ratios\n",
            "Action: Calculator\n",
            "Action Input: 0.08228117463469994 * 0.08228117463469994\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 0.00677019169926599\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Spent a total of 2643 tokens\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    zero_shot_agent, \n",
        "    \"What is the multiplication of the ratio between stock \" +\n",
        "    \"prices for 'ABC' and 'XYZ' in January 3rd and the ratio \" + \n",
        "    \"between the same stock prices in January the 4th?\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b8b23c42",
      "metadata": {
        "id": "b8b23c42"
      },
      "source": [
        "As always, let's see what the prompt is here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "3e50e81b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e50e81b",
        "outputId": "c1fb3eb0-01b5-46e1-f17b-d1df2bff2df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Calculator: Useful for when you need to answer questions about math.\n",
            "Stock DB: Useful for when you need to answer questions about stocks and their prices.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Calculator, Stock DB]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: {input}\n",
            "Thought:{agent_scratchpad}\n"
          ]
        }
      ],
      "source": [
        "print(zero_shot_agent.agent.llm_chain.prompt.template)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5b2b47ed",
      "metadata": {
        "id": "5b2b47ed"
      },
      "source": [
        "The question we must ask ourselves here is: how are agents different than chains?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f73aaf07",
      "metadata": {
        "id": "f73aaf07"
      },
      "source": [
        "If we look at the agent's logic and the prompt we have just printed we will see some clear differences. First, we have the tools which are included in the prompt. Second we have a thought process which was before was immediate in chains but now involves a 'thought', 'action', 'action input', 'observation' sequence. What is this all about?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3a698f78",
      "metadata": {
        "id": "3a698f78"
      },
      "source": [
        "Suffice it to say for now that **the LLM now has the ability to 'reason' on how to best use tools** to solve our query and can combine them in intelligent ways with just a brief description of each of them. If you want to learn more about this paradigm (MRKL) in detail, please refer to [this](https://arxiv.org/pdf/2205.00445.pdf) paper. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aff9e65d",
      "metadata": {
        "id": "aff9e65d"
      },
      "source": [
        "Finally, let's pay attention to the 'agent_scratchpad'. What is that? Well, that is where we will be appending every thought or action that the agent has already performed. In this way, at each point in time, the agent will know what it has found out and will be able to continue its thought process. In other words, after using a tool it adds its thoughts and observations to the scratchpad and picks up from there."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "04d70642",
      "metadata": {
        "id": "04d70642"
      },
      "source": [
        "### Agent type #2: Conversational React"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1ec589b1",
      "metadata": {
        "id": "1ec589b1"
      },
      "source": [
        "The zero shot agent is really interesting but, as we said before, it has no memory. What if we want an assistant that remembers things we have talked about and can also reason about them and use tools? For that we have the conversational react agent."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5c26c50f",
      "metadata": {
        "id": "5c26c50f"
      },
      "source": [
        "We will use the same tools as we specified earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "11ae1e8e",
      "metadata": {
        "id": "11ae1e8e"
      },
      "outputs": [],
      "source": [
        "tools = load_tools(\n",
        "    [\"llm-math\"], \n",
        "    llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "4b6faff3",
      "metadata": {
        "id": "4b6faff3"
      },
      "outputs": [],
      "source": [
        "tools.append(sql_tool)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "be45b903",
      "metadata": {
        "id": "be45b903"
      },
      "source": [
        "The memory type being used here is a simple buffer memory to allow us to remember previous steps in the reasoning chain. For more information on memory, please refer to the 3rd chapter of this series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "0aff4edf",
      "metadata": {
        "id": "0aff4edf"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "6579cef0",
      "metadata": {
        "id": "6579cef0"
      },
      "outputs": [],
      "source": [
        "conversational_agent = initialize_agent(\n",
        "    agent='conversational-react-description', \n",
        "    tools=tools, \n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    memory=memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "cabbea50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cabbea50",
        "outputId": "6bdd0b13-134f-4dfd-9fb7-66c746e5a07a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: Stock DB\n",
            "Action Input: ABC on January the 1st\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "ABC on January the 1st\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-01'\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(200.0,)]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mThe price of ABC on January the 1st was 200.0.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mThe price of ABC on January the 1st was 200.0.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
            "AI: The price of ABC on January the 1st was 200.0. Is there anything else I can help you with?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Spent a total of 1847 tokens\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversational_agent, \n",
        "    \"Please provide me the stock prices for ABC on January the 1st\"\n",
        "                     )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bdcd9847",
      "metadata": {
        "id": "bdcd9847"
      },
      "source": [
        "As we can see below, the prompt is similar but it includes a great prelude of instructions that make it an effective assistant as well + a spot for including the chat history from the memory component:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "de413fd3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de413fd3",
        "outputId": "faf8eeb7-94df-4e92-f1a8-dfde49d1703c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant is a large language model trained by OpenAI.\n",
            "\n",
            "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
            "\n",
            "TOOLS:\n",
            "------\n",
            "\n",
            "Assistant has access to the following tools:\n",
            "\n",
            "> Calculator: Useful for when you need to answer questions about math.\n",
            "> Stock DB: Useful for when you need to answer questions about stocks and their prices.\n",
            "\n",
            "To use a tool, please use the following format:\n",
            "\n",
            "```\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: the action to take, should be one of [Calculator, Stock DB]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "```\n",
            "\n",
            "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
            "\n",
            "```\n",
            "Thought: Do I need to use a tool? No\n",
            "AI: [your response here]\n",
            "```\n",
            "\n",
            "Begin!\n",
            "\n",
            "Previous conversation history:\n",
            "{chat_history}\n",
            "\n",
            "New input: {input}\n",
            "{agent_scratchpad}\n"
          ]
        }
      ],
      "source": [
        "print(conversational_agent.agent.llm_chain.prompt.template)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "32ee8041",
      "metadata": {
        "id": "32ee8041"
      },
      "source": [
        "The conversational assitant is towards conversation and has a bit more trouble combining a complex combination of tools. Let's see what happens if we make it try and answer the same question we posed to our previous agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "5e109878",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e109878",
        "outputId": "6d7cab70-338e-4d88-9257-f34dee165514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: Calculator\n",
            "Action Input: The multiplication of the ratio of the prices of stocks 'ABC' and 'XYZ' in January 3rd\u001b[0m"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "LLMMathChain._evaluate(\"\n(ABC_price_Jan3rd / XYZ_price_Jan3rd) * 100\n\") raised error: 'ABC_price_Jan3rd'. Please try again with a valid numerical expression",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\numexpr\\necompiler.py:743\u001b[0m, in \u001b[0;36mgetArguments\u001b[1;34m(names, local_dict, global_dict)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 743\u001b[0m     a \u001b[39m=\u001b[39m local_dict[name]\n\u001b[0;32m    744\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
            "\u001b[1;31mKeyError\u001b[0m: 'ABC_price_Jan3rd'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\llm_math\\base.py:80\u001b[0m, in \u001b[0;36mLLMMathChain._evaluate_expression\u001b[1;34m(self, expression)\u001b[0m\n\u001b[0;32m     78\u001b[0m     local_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpi\u001b[39m\u001b[39m\"\u001b[39m: math\u001b[39m.\u001b[39mpi, \u001b[39m\"\u001b[39m\u001b[39me\u001b[39m\u001b[39m\"\u001b[39m: math\u001b[39m.\u001b[39me}\n\u001b[0;32m     79\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m---> 80\u001b[0m         numexpr\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m     81\u001b[0m             expression\u001b[39m.\u001b[39;49mstrip(),\n\u001b[0;32m     82\u001b[0m             global_dict\u001b[39m=\u001b[39;49m{},  \u001b[39m# restrict access to globals\u001b[39;49;00m\n\u001b[0;32m     83\u001b[0m             local_dict\u001b[39m=\u001b[39;49mlocal_dict,  \u001b[39m# add common mathematical functions\u001b[39;49;00m\n\u001b[0;32m     84\u001b[0m         )\n\u001b[0;32m     85\u001b[0m     )\n\u001b[0;32m     86\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\numexpr\\necompiler.py:819\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(ex, local_dict, global_dict, out, order, casting, **kwargs)\u001b[0m\n\u001b[0;32m    818\u001b[0m names, ex_uses_vml \u001b[39m=\u001b[39m _names_cache[expr_key]\n\u001b[1;32m--> 819\u001b[0m arguments \u001b[39m=\u001b[39m getArguments(names, local_dict, global_dict)\n\u001b[0;32m    821\u001b[0m \u001b[39m# Create a signature\u001b[39;00m\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\numexpr\\necompiler.py:745\u001b[0m, in \u001b[0;36mgetArguments\u001b[1;34m(names, local_dict, global_dict)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m     a \u001b[39m=\u001b[39m global_dict[name]\n\u001b[0;32m    746\u001b[0m arguments\u001b[39m.\u001b[39mappend(numpy\u001b[39m.\u001b[39masarray(a))\n",
            "\u001b[1;31mKeyError\u001b[0m: 'ABC_price_Jan3rd'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m count_tokens(\n\u001b[0;32m      2\u001b[0m     conversational_agent, \n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mWhat is the multiplication of the ratio of the prices of stocks \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mABC\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m and \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mXYZ\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m in January 3rd?   And what is the ratio for January the 4th?\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m )\n",
            "Cell \u001b[1;32mIn[38], line 5\u001b[0m, in \u001b[0;36mcount_tokens\u001b[1;34m(agent, query)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount_tokens\u001b[39m(agent, query):\n\u001b[0;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m get_openai_callback() \u001b[39mas\u001b[39;00m cb:\n\u001b[1;32m----> 5\u001b[0m         result \u001b[39m=\u001b[39m agent(query)\n\u001b[0;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSpent a total of \u001b[39m\u001b[39m{\u001b[39;00mcb\u001b[39m.\u001b[39mtotal_tokens\u001b[39m}\u001b[39;00m\u001b[39m tokens\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 243\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    244\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    245\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    247\u001b[0m )\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    231\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    232\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    233\u001b[0m     inputs,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 237\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    238\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    239\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\agents\\agent.py:987\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m--> 987\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[0;32m    988\u001b[0m         name_to_tool_map,\n\u001b[0;32m    989\u001b[0m         color_mapping,\n\u001b[0;32m    990\u001b[0m         inputs,\n\u001b[0;32m    991\u001b[0m         intermediate_steps,\n\u001b[0;32m    992\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[0;32m    993\u001b[0m     )\n\u001b[0;32m    994\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m    995\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[0;32m    996\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[0;32m    997\u001b[0m         )\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\agents\\agent.py:850\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    848\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    849\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[1;32m--> 850\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m    851\u001b[0m         agent_action\u001b[39m.\u001b[39;49mtool_input,\n\u001b[0;32m    852\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    853\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[0;32m    854\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_run_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    858\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\tools\\base.py:320\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    319\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 320\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    321\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[0;32m    323\u001b[0m         \u001b[39mstr\u001b[39m(observation), color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    324\u001b[0m     )\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\tools\\base.py:292\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     tool_args, tool_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    291\u001b[0m     observation \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 292\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49mtool_args, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_kwargs)\n\u001b[0;32m    293\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    294\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39mtool_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs)\n\u001b[0;32m    295\u001b[0m     )\n\u001b[0;32m    296\u001b[0m \u001b[39mexcept\u001b[39;00m ToolException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    297\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_tool_error:\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\tools\\base.py:444\u001b[0m, in \u001b[0;36mTool._run\u001b[1;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m new_argument_supported \u001b[39m=\u001b[39m signature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    443\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 444\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\n\u001b[0;32m    445\u001b[0m         \u001b[39m*\u001b[39;49margs,\n\u001b[0;32m    446\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    447\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    448\u001b[0m     )\n\u001b[0;32m    449\u001b[0m     \u001b[39mif\u001b[39;00m new_argument_supported\n\u001b[0;32m    450\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    451\u001b[0m )\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:440\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    439\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[0;32m    441\u001b[0m         _output_key\n\u001b[0;32m    442\u001b[0m     ]\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[0;32m    446\u001b[0m         _output_key\n\u001b[0;32m    447\u001b[0m     ]\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 243\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    244\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    245\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    247\u001b[0m )\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    231\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    232\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    233\u001b[0m     inputs,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 237\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    238\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    239\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\llm_math\\base.py:149\u001b[0m, in \u001b[0;36mLLMMathChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    143\u001b[0m _run_manager\u001b[39m.\u001b[39mon_text(inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key])\n\u001b[0;32m    144\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(\n\u001b[0;32m    145\u001b[0m     question\u001b[39m=\u001b[39minputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key],\n\u001b[0;32m    146\u001b[0m     stop\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m```output\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    147\u001b[0m     callbacks\u001b[39m=\u001b[39m_run_manager\u001b[39m.\u001b[39mget_child(),\n\u001b[0;32m    148\u001b[0m )\n\u001b[1;32m--> 149\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_llm_result(llm_output, _run_manager)\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\llm_math\\base.py:103\u001b[0m, in \u001b[0;36mLLMMathChain._process_llm_result\u001b[1;34m(self, llm_output, run_manager)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m text_match:\n\u001b[0;32m    102\u001b[0m     expression \u001b[39m=\u001b[39m text_match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_expression(expression)\n\u001b[0;32m    104\u001b[0m     run_manager\u001b[39m.\u001b[39mon_text(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAnswer: \u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    105\u001b[0m     run_manager\u001b[39m.\u001b[39mon_text(output, color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myellow\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
            "File \u001b[1;32mc:\\repos\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\llm_math\\base.py:87\u001b[0m, in \u001b[0;36mLLMMathChain._evaluate_expression\u001b[1;34m(self, expression)\u001b[0m\n\u001b[0;32m     79\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[0;32m     80\u001b[0m         numexpr\u001b[39m.\u001b[39mevaluate(\n\u001b[0;32m     81\u001b[0m             expression\u001b[39m.\u001b[39mstrip(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m         )\n\u001b[0;32m     85\u001b[0m     )\n\u001b[0;32m     86\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 87\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLLMMathChain._evaluate(\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mexpression\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m) raised error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Please try again with a valid numerical expression\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     )\n\u001b[0;32m     92\u001b[0m \u001b[39m# Remove any leading and trailing brackets from the output\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m]$\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, output)\n",
            "\u001b[1;31mValueError\u001b[0m: LLMMathChain._evaluate(\"\n(ABC_price_Jan3rd / XYZ_price_Jan3rd) * 100\n\") raised error: 'ABC_price_Jan3rd'. Please try again with a valid numerical expression"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversational_agent, \n",
        "    \"What is the multiplication of the ratio of the prices of stocks 'ABC' and 'XYZ' in January 3rd?   And what is the ratio for January the 4th?\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fe11c646",
      "metadata": {
        "id": "fe11c646"
      },
      "source": [
        "As we can see, it solves the problem but in a different manner, by using the SQL tool to solve the math as well. Interesting!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44135b8d",
      "metadata": {
        "id": "44135b8d"
      },
      "source": [
        "### Agent type #3: React Docstore"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1e6d0d13",
      "metadata": {
        "id": "1e6d0d13"
      },
      "source": [
        "This type of agent is similar to the ones we have seen so far but it includes the interaction with a docstore. It will have two and only two tools at its disposal: 'Search' and 'Lookup'."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d569fd41",
      "metadata": {
        "id": "d569fd41"
      },
      "source": [
        "With 'Search' it will bring up a relevant article and with 'Lookup' the agent will find the right piece of information in the article. This is probably easiest to see in an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc452af",
      "metadata": {
        "id": "ecc452af"
      },
      "outputs": [],
      "source": [
        "from langchain import Wikipedia\n",
        "from langchain.agents.react.base import DocstoreExplorer\n",
        "\n",
        "docstore=DocstoreExplorer(Wikipedia())\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=docstore.search,\n",
        "        description='search wikipedia'\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Lookup\",\n",
        "        func=docstore.lookup,\n",
        "        description='lookup a term in wikipedia'\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595938a1",
      "metadata": {
        "id": "595938a1"
      },
      "outputs": [],
      "source": [
        "docstore_agent = initialize_agent(\n",
        "    tools, \n",
        "    llm, \n",
        "    agent=\"react-docstore\", \n",
        "    verbose=True,\n",
        "    max_iterations=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba6b065",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bba6b065",
        "outputId": "40179869-4d21-432c-fba3-2f3f548b3034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to search Archimedes and find his last words.\n",
            "Action: Search[Archimedes]\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mArchimedes of Syracuse (; c. 287 – c. 212 BC) was an Ancient Greek mathematician, physicist, engineer, astronomer, and inventor from the ancient city of Syracuse in Sicily. Although few details of his life are known, he is regarded as one of the leading scientists in classical antiquity. Considered the greatest mathematician of ancient history, and one of the greatest of all time, Archimedes anticipated modern calculus and analysis by applying the concept of the infinitely small and the method of exhaustion to derive and rigorously prove a range of geometrical theorems. These include the area of a circle, the surface area and volume of a sphere, the area of an ellipse, the area under a parabola, the volume of a segment of a paraboloid of revolution, the volume of a segment of a hyperboloid of revolution, and the area of a spiral.Archimedes' other mathematical achievements include deriving an approximation of pi, defining and investigating the Archimedean spiral, and devising a system using exponentiation for expressing very large numbers. He was also one of the first to apply mathematics to physical phenomena, working on statics and hydrostatics. Archimedes' achievements in this area include a proof of the law of the lever, the widespread use of the concept of center of gravity, and the enunciation of the law of buoyancy or Archimedes' principle. He is also credited with designing innovative machines, such as his screw pump, compound pulleys, and defensive war machines to protect his native Syracuse from invasion.\n",
            "Archimedes died during the siege of Syracuse, when he was killed by a Roman soldier despite orders that he should not be harmed. Cicero describes visiting Archimedes' tomb, which was surmounted by a sphere and a cylinder that Archimedes requested be placed there to represent his mathematical discoveries.\n",
            "Unlike his inventions, Archimedes' mathematical writings were little known in antiquity. Mathematicians from Alexandria read and quoted him, but the first comprehensive compilation was not made until c. 530 AD by Isidore of Miletus in Byzantine Constantinople, while commentaries on the works of Archimedes by Eutocius in the 6th century opened them to wider readership for the first time. The relatively few copies of Archimedes' written work that survived through the Middle Ages were an influential source of ideas for scientists during the Renaissance and again in the 17th century, while the discovery in 1906 of previously lost works by Archimedes in the Archimedes Palimpsest has provided new insights into how he obtained mathematical results.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m The paragraph does not mention Archimedes' last words. I need to look up \"last words\".\n",
            "Action: Lookup[last words]\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m(Result 1/1) Plutarch (45–119 AD) wrote in his Parallel Lives that Archimedes was related to King Hiero II, the ruler of Syracuse. He also provides at least two accounts on how Archimedes died after the city was taken. According to the most popular account, Archimedes was contemplating a mathematical diagram when the city was captured. A Roman soldier commanded him to come and meet Marcellus, but he declined, saying that he had to finish working on the problem. This enraged the soldier, who killed Archimedes with his sword. Another story has Archimedes carrying mathematical instruments before being killed because a soldier thought they were valuable items. Marcellus was reportedly angered by Archimedes' death, as he considered him a valuable scientific asset (he called Archimedes \"a geometrical Briareus\") and had ordered that he should not be harmed.The last words attributed to Archimedes are \"Do not disturb my circles\" (Latin, \"Noli turbare circulos meos\"; Katharevousa Greek, \"μὴ μου τοὺς κύκλους τάραττε\"), a reference to the mathematical drawing that he was supposedly studying when disturbed by the Roman soldier. There is no reliable evidence that Archimedes uttered these words and they do not appear in Plutarch's account. A similar quotation is found in the work of Valerius Maximus (fl. 30 AD), who wrote in Memorable Doings and Sayings, \"... sed protecto manibus puluere 'noli' inquit, 'obsecro, istum disturbare'\" (\"... but protecting the dust with his hands, said 'I beg of you, do not disturb this'\").\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m The last words attributed to Archimedes are \"Do not disturb my circles\", so the answer is \"Do not disturb my circles\".\n",
            "Action: Finish[\"Do not disturb my circles\"]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Spent a total of 6066 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': \"What were Archimedes' last words?\",\n",
              " 'output': '\"Do not disturb my circles\"'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(docstore_agent, \"What were Archimedes' last words?\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6a5f77ac",
      "metadata": {
        "id": "6a5f77ac"
      },
      "source": [
        "We will not print the prompt here because it is too large, but you can see it yourself if you want to (we know how to already).\n",
        "\n",
        "In short, it contains several examples of the `Question` > `Thought` > `Action` > `Observation` loop, that include the `Search` and `Lookup` tools.\n",
        "\n",
        "If you want to learn more about this approach [this](https://arxiv.org/pdf/2210.03629.pdf) is the paper for ReAct"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4aae3b5b",
      "metadata": {
        "id": "4aae3b5b"
      },
      "source": [
        "### Agent type #4: Self Ask with Search"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c07039e5",
      "metadata": {
        "id": "c07039e5"
      },
      "source": [
        "This is the first-choice agent to use when using LLM's to extract information with a search engine. The agent will ask follow-up questions and use the search functionality to get intermediate answers that help it get to a final answer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903660b2",
      "metadata": {
        "id": "903660b2"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI, SerpAPIWrapper\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "\n",
        "search = SerpAPIWrapper(serpapi_api_key='api_key')\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Intermediate Answer\",\n",
        "        func=search.run,\n",
        "        description='google search'\n",
        "    )\n",
        "]\n",
        "\n",
        "self_ask_with_search = initialize_agent(tools, llm, agent=\"self-ask-with-search\", verbose=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0ec1c4c8",
      "metadata": {
        "id": "0ec1c4c8"
      },
      "source": [
        "We will not interact with this agent because for that we would need a serpapi key. However, by checking out the prompt we can see a few examples of how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4134930",
      "metadata": {
        "id": "e4134930",
        "outputId": "eed4d667-5e20-4db8-e8e2-75313c022928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: How old was Muhammad Ali when he died?\n",
            "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
            "Follow up: How old was Alan Turing when he died?\n",
            "Intermediate answer: Alan Turing was 41 years old when he died.\n",
            "So the final answer is: Muhammad Ali\n",
            "\n",
            "Question: When was the founder of craigslist born?\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: Who was the founder of craigslist?\n",
            "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
            "Follow up: When was Craig Newmark born?\n",
            "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
            "So the final answer is: December 6, 1952\n",
            "\n",
            "Question: Who was the maternal grandfather of George Washington?\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: Who was the mother of George Washington?\n",
            "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
            "Follow up: Who was the father of Mary Ball Washington?\n",
            "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
            "So the final answer is: Joseph Ball\n",
            "\n",
            "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: Who is the director of Jaws?\n",
            "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
            "Follow up: Where is Steven Spielberg from?\n",
            "Intermediate answer: The United States.\n",
            "Follow up: Who is the director of Casino Royale?\n",
            "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
            "Follow up: Where is Martin Campbell from?\n",
            "Intermediate answer: New Zealand.\n",
            "So the final answer is: No\n",
            "\n",
            "Question: {input}\n",
            "Are followup questions needed here:{agent_scratchpad}\n"
          ]
        }
      ],
      "source": [
        "print(self_ask_with_search.agent.llm_chain.prompt.template)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3e7790ae",
      "metadata": {
        "id": "3e7790ae"
      },
      "source": [
        "As we can see, the prompt is basically a series of many examples to show the LLM how to ask follow up questions to a search tool until it can get to the final answer."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5df324ba",
      "metadata": {
        "id": "5df324ba"
      },
      "source": [
        "And.. again [here](https://arxiv.org/pdf/2210.03350.pdf) you have the paper to dive deeper!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4cd80a08",
      "metadata": {
        "id": "4cd80a08"
      },
      "source": [
        "### Wrapping up"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "97d108e2",
      "metadata": {
        "id": "97d108e2"
      },
      "source": [
        "And that all for agents! There's many other things you can do with agents, just to name a few:\n",
        "* Create your own custom agent\n",
        "* Use them with many other tools (even custom ones)\n",
        "* Trace every call an agent makes through a convinient UI interface"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e33dd90a",
      "metadata": {
        "id": "e33dd90a"
      },
      "source": [
        "Check the how-to [guides](https://langchain.readthedocs.io/en/latest/modules/agents/how_to_guides.html) for more!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
